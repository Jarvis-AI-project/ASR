{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'whisper.cpp'...\n",
      "remote: Enumerating objects: 3909, done.\u001b[K\n",
      "remote: Counting objects: 100% (1795/1795), done.\u001b[K\n",
      "remote: Compressing objects: 100% (540/540), done.\u001b[K\n",
      "remote: Total 3909 (delta 1439), reused 1398 (delta 1226), pack-reused 2114\u001b[K\n",
      "Receiving objects: 100% (3909/3909), 6.76 MiB | 6.12 MiB/s, done.\n",
      "Resolving deltas: 100% (2448/2448), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/whisper.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ggml model large from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
      "Model large already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "!cd whisper.cpp && \\\n",
    "    bash ./models/download-ggml-model.sh large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_no_state: loading model from 'models/ggml-large.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 5\n",
      "whisper_model_load: mem required  = 3557.00 MB (+   71.00 MB per decoder)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: model ctx     = 2951.27 MB\n",
      "whisper_model_load: model size    = 2950.66 MB\n",
      "whisper_init_state: kv self size  =   70.00 MB\n",
      "whisper_init_state: kv cross size =  234.38 MB\n",
      "\n",
      "system_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | COREML = 0 | \n",
      "\n",
      "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n",
      "\n",
      "[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n",
      "\n",
      "\n",
      "whisper_print_timings:     load time =  2050.82 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =   171.41 ms\n",
      "whisper_print_timings:   sample time =    10.70 ms /    27 runs (    0.40 ms per run)\n",
      "whisper_print_timings:   encode time = 32163.93 ms /     1 runs (32163.93 ms per run)\n",
      "whisper_print_timings:   decode time =  2722.14 ms /    27 runs (  100.82 ms per run)\n",
      "whisper_print_timings:    total time = 37302.30 ms\n"
     ]
    }
   ],
   "source": [
    "!cd whisper.cpp && \\\n",
    "    ./main -m models/ggml-large.bin -f samples/jfk.wav"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
