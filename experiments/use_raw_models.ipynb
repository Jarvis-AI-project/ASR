{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains code to use raw whisper model using various methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Whisper Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_PATH = '../models/'\n",
    "_MODEL_NAME = 'large-v3'\n",
    "_AUDIO_PATH = './audio_samples/test.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.whisper import load_model, load_audio, pad_or_trim, log_mel_spectrogram, DecodingOptions, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\n",
    "    name=_MODEL_NAME,\n",
    "    download_root=_MODEL_PATH,\n",
    "    device='cuda:0',\n",
    "    in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = load_audio(_AUDIO_PATH)\n",
    "audio = pad_or_trim(audio)\n",
    "mel = log_mel_spectrogram(audio, n_mels=128).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world, this is a sample test for the python post request module to the whisper API hosted on the jarvis server Now I am going to speak in Hindi So I have opened my whatsapp and I have messaged a person that he will participate in the event Thank you very much'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = DecodingOptions(language='en')\n",
    "result = decode(model, mel, options)\n",
    "result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_PATH = '../models/whisper-large-v3/'\n",
    "_AUDIO_PATH = './audio_samples/test2.wav'\n",
    "LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperConfig,\n",
    ")\n",
    "import torch\n",
    "import ffmpeg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_audio and pad_or_trim functions\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_LENGTH = 30  # 30-second chunks\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio = whisper.load_audio('test.wav')\n",
    "def load_audio(file: str, sr: int = SAMPLE_RATE, start_time: int = 0, dtype=np.float16):\n",
    "    \"\"\"\n",
    "    Load an audio file into a numpy array at the specified sampling rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, ss=start_time, threads=0)\n",
    "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except ffmpeg.Error as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    # return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(dtype) / 32768.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio = whisper.pad_or_trim(audio)\n",
    "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(\n",
    "                dim=axis, index=torch.arange(length, device=array.device)\n",
    "            )\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(_MODEL_PATH)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "config = WhisperConfig.from_pretrained(_MODEL_PATH)\n",
    "\n",
    "model = WhisperForConditionalGeneration(config=config).from_pretrained(\n",
    "    _MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # use_flash_attention_2=True\n",
    ").to('cuda:0')\n",
    "# model.to_bettertransformer()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = load_audio(_AUDIO_PATH)\n",
    "audio = pad_or_trim(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 3000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = processor(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\").input_features.to('cuda:0')\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.6641801890000352\n",
      "And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(\n",
    "        input_features,\n",
    "        # num_beams=4,\n",
    "        language=LANGUAGE,\n",
    "        task=\"transcribe\",\n",
    "        use_cache=True,\n",
    "        is_multilingual=True,\n",
    "        return_timestamps=True,\n",
    "    )\n",
    "transcription = tokenizer.batch_decode(\n",
    "    predicted_ids, \n",
    "    skip_special_tokens=True,\n",
    ")[0]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f'Inference time: {end_time - start_time}')\n",
    "print(transcription.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
